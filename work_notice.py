import os
import time
import requests
import yaml
import logging
import asyncio
import aiohttp
from datetime import datetime
from bs4 import BeautifulSoup
from logging.handlers import TimedRotatingFileHandler
from pg_config_connect import get_pg_pool

# ---------------------------
# ì‚¬ìš©ì ì •ì˜ ë‚ ì§œ ê¸°ë°˜ FileHandler
# ---------------------------
class DateBasedFileHandler(logging.FileHandler):
    def __init__(self, directory, filename_format, encoding=None, delay=False):
        self.directory = directory
        self.filename_format = filename_format
        self.current_date = datetime.now().strftime("%Y_%m_%d")
        if not os.path.exists(self.directory):
            os.makedirs(self.directory)
        filename = os.path.join(self.directory, self.filename_format.format(date=self.current_date))
        super().__init__(filename, mode="a", encoding=encoding, delay=delay)

    def emit(self, record):
        new_date = datetime.now().strftime("%Y_%m_%d")
        if new_date != self.current_date:
            self.current_date = new_date
            self.acquire()
            try:
                self.close()
                new_filename = os.path.join(self.directory, self.filename_format.format(date=new_date))
                self.baseFilename = os.path.abspath(new_filename)
                self.stream = self._open()
            finally:
                self.release()
        super().emit(record)

# ---------------------------
# ë¡œê·¸ ì„¤ì •
# ---------------------------
log_formatter = logging.Formatter('%(asctime)s - [%(levelname)s] - %(message)s')
logger = logging.getLogger()
logger.setLevel(logging.INFO)

console_handler = logging.StreamHandler()
console_handler.setFormatter(log_formatter)
logger.addHandler(console_handler)

file_handler = DateBasedFileHandler("log", "{date}_work.log", encoding="utf-8")
file_handler.setFormatter(log_formatter)
logger.addHandler(file_handler)

# ---------------------------
# ì„¤ì • ë¡œë“œ
# ---------------------------
with open('config.yaml', encoding='UTF-8') as f:
    config = yaml.load(f, Loader=yaml.FullLoader)
DISCORD_WEBHOOK_URL = config['DISCORD_WEBHOOK_URL']

# ---------------------------
# í‚¤ì›Œë“œ DB ì¡°íšŒ í•¨ìˆ˜
# ---------------------------
async def get_keywords():
    pool = await get_pg_pool()
    try:
        keywords_list = await pool.fetch("SELECT keyword FROM notice_bot.work_keywords")
        keywords_list = [row['keyword'].lower() for row in keywords_list]
        logging.info(f"DBì—ì„œ ê°€ì ¸ì˜¨ í‚¤ì›Œë“œ: {keywords_list}")
        return keywords_list
    finally:
        await pool.close()

# ---------------------------
# ë””ìŠ¤ì½”ë“œ ì›¹í›… ì „ì†¡
# ---------------------------
async def send_discord_alert(message_content):
    message = {"content": message_content}
    async with aiohttp.ClientSession() as session:
        async with session.post(DISCORD_WEBHOOK_URL, data=message) as response:
            if response.status not in (200, 204):
                logging.error(f"ë””ìŠ¤ì½”ë“œ ì›¹í›… ì „ì†¡ ì‹¤íŒ¨: {response.status}")
            else:
                logging.info("ë””ìŠ¤ì½”ë“œ ì›¹í›… ì „ì†¡ ì„±ê³µ")

# ---------------------------
# ì±„ìš© ì •ë³´ í¬ë¡¤ë§
# ---------------------------
async def fetch_work_data(url):
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            if response.status != 200:
                logging.error(f"URL ìš”ì²­ ì‹¤íŒ¨, ìƒíƒœ ì½”ë“œ: {response.status} / URL: {url}")
                return []
            
            html = await response.text()
            soup = BeautifulSoup(html, 'html.parser')
            table_ul = soup.find('ul', {'data-role': 'table', 'class': 'black'})
            if not table_ul:
                logging.error("ì§€ì •í•œ í…Œì´ë¸” ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []

            tbody_items = table_ul.select("li.tbody, li.tbody.notice")
            data_list = []

            for item in tbody_items:
                loopnum_tag = item.find('span', class_='loopnum')
                if not loopnum_tag:
                    continue
                record_id = loopnum_tag.get_text(strip=True)
                org_span = item.find('span', class_='org')
                company_text = org_span.get_text(strip=True) if org_span else ""
                title_span = item.find('span', class_='title')
                if title_span:
                    a_tag = title_span.find('a')
                    if a_tag:
                        href = a_tag.get('href')
                        full_url = href if href.startswith("http") else "https://career.hansung.ac.kr" + href
                        title_text = a_tag.get_text(strip=True)
                    else:
                        full_url, title_text = "", ""
                else:
                    full_url, title_text = "", ""
                deadline_span = item.find('span', class_='deadline center')
                deadline_text = deadline_span.get_text(strip=True) if deadline_span else ""

                data_list.append({
                    "id": record_id,
                    "company": company_text,
                    "title": title_text,
                    "url": full_url,
                    "deadline": deadline_text
                })

            return data_list

async def fetch_work_recommend():
    return await fetch_work_data("https://career.hansung.ac.kr/ko/recruitment/recommend")

async def fetch_work_general():
    return await fetch_work_data("https://career.hansung.ac.kr/ko/recruitment/general")

# ---------------------------
# í‚¤ì›Œë“œ ê²€ì‚¬ ë° DB ì €ì¥
# ---------------------------
async def check_work_type(url, keywords):
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                if response.status != 200:
                    logging.error(f"ê²Œì‹œê¸€ URL ìš”ì²­ ì‹¤íŒ¨ (ìƒíƒœ ì½”ë“œ: {response.status})")
                    return None
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                post_article = soup.find("article", {"data-role": "post"})
                if post_article:
                    content_text = post_article.get_text().lower()
                    for keyword in keywords:
                        if keyword in content_text:
                            return "developer"
                return None
    except Exception as e:
        logging.error(f"ê²Œì‹œê¸€ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return None

async def store_data_to_db(data_list, table_name):
    pool = await get_pg_pool()
    try:
        current_timestamp = datetime.now()
        keywords = await get_keywords()

        for record in data_list:
            record_key = f"{record['id']}_{record['company']}" if record['company'] else record['id']
            
            # ì´ë¯¸ ì €ì¥ëœ ê²Œì‹œê¸€ì¸ì§€ í™•ì¸
            row = await pool.fetchrow(f"SELECT 1 FROM notice_bot.{table_name} WHERE record_key = $1", record_key)
            if row:
                logging.info(f"ì´ë¯¸ ì €ì¥ëœ ê²Œì‹œê¸€ - KEY: {record_key}")
                continue

            # ê²Œì‹œê¸€ íƒ€ì… í™•ì¸
            work_type = await check_work_type(record["url"], keywords=keywords)
            
            # ë°ì´í„° ì‚½ì…
            await pool.execute(f"""
                INSERT INTO notice_bot.{table_name} (record_key, time, title, url, company, deadline, work_type)
                VALUES ($1, $2, $3, $4, $5, $6, $7)
            """, record_key, current_timestamp, record["title"], record["url"], record["company"], record["deadline"], work_type)

            logging.info(f"ì €ì¥ ì™„ë£Œ - KEY: {record_key}, ì œëª©: {record['title']}, URL: {record['url']}")

            # ê°œë°œì ê´€ë ¨ ê³µê³ ì¸ ê²½ìš° ë””ìŠ¤ì½”ë“œ ì•Œë¦¼
            if work_type:
                header = "ğŸ“ğŸŸ¢ìƒˆë¡œìš´ ì¶”ì²œì±„ìš© ê³µê³ " if "recommend" in record["url"] else "ğŸ“ğŸ”µìƒˆë¡œìš´ ì¼ë°˜ì±„ìš© ê³µê³ "
                discord_message = f"""
{header}
íšŒì‚¬: {record['company']}
ì œëª©: {record['title']}
ë§ˆê°ì¼: {record['deadline']}
URL: {record['url']}
"""
                await send_discord_alert(discord_message)
                await asyncio.sleep(1)
    finally:
        await pool.close()

# ---------------------------
# ë©”ì¸ ì‹¤í–‰
# ---------------------------
async def main_async():
    while True:
        # ì¶”ì²œ ì±„ìš© í˜ì´ì§€ í¬ë¡¤ë§
        recommend_list = await fetch_work_recommend()
        if recommend_list:
            logging.info("ì¶”ì²œ ì±„ìš© í˜ì´ì§€ - ì¶”ì¶œëœ ë°ì´í„°:")
            for record in recommend_list:
                logging.info(record)
            await store_data_to_db(recommend_list, 'work_recommend')
        else:
            logging.warning("ì¶”ì²œ ì±„ìš© í˜ì´ì§€ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        # ì¼ë°˜ ì±„ìš© í˜ì´ì§€ í¬ë¡¤ë§
        general_list = await fetch_work_general()
        if general_list:
            logging.info("ì¼ë°˜ ì±„ìš© í˜ì´ì§€ - ì¶”ì¶œëœ ë°ì´í„°:")
            for record in general_list:
                logging.info(record)
            await store_data_to_db(general_list, 'work_general')
        else:
            logging.warning("ì¼ë°˜ ì±„ìš© í˜ì´ì§€ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        logging.info("5ë¶„ ëŒ€ê¸° ì¤‘...")
        await asyncio.sleep(300)

def main():
    # ë¹„ë™ê¸° ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
    asyncio.run(main_async())

if __name__ == "__main__":
    main()
